name: ETL Diario Postgres a BigQuery

# Se ejecuta todos los días a las 12:00 UTC (8:00 AM Chile aprox)
on:
  schedule:
    - cron: '0 12 * * *'
  # También permite ejecutarlo manualmente desde la pestaña "Actions"
  workflow_dispatch:

jobs:
  run-etl:
    runs-on: ubuntu-latest

    steps:
      # 1. Descargar tu código del repo
      - name: Checkout code
        uses: actions/checkout@v3

      # 2. Instalar Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      # 3. Instalar librerías
      - name: Install dependencies
        run: |
          pip install pandas sqlalchemy psycopg2-binary google-cloud-bigquery db-dtypes

      # 4. Crear el archivo de credenciales de Google desde el Secreto
      - name: Create Google Credentials File
        run: |
          echo '${{ secrets.GCP_SA_KEY }}' > gcp_key.json

      # 5. Ejecutar el Script
      - name: Run ETL Script
        env:
          # Pasamos los secretos como variables de entorno al script
          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
          DB_HOST: ${{ secrets.DB_HOST }}
          GOOGLE_APPLICATION_CREDENTIALS: ./gcp_key.json
        run: python import_pandas_as_pd.py